{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoI4g81U8Hu_"
   },
   "source": [
    "# Image Directory\n",
    "We've provided a few images from ImageNet. If working locally, update ```DATA_FOLDER``` based on your local directory. Based on the documentation for [ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html), within ```DATA_FOLDER``` you should have one folder for each class, respectively ```'bagel', 'barn', 'goldfish', 'mud_turtle'```. Put each provided image into the respective folder.\n",
    "\n",
    "We've also provided a file that lists the class names of ImageNet, in order. Update ```CLASSNAMES_FILE``` based on your local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XwasrUT18ZbY"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './imagenet-images/'\n",
    "CLASSNAMES_FILE = './imagenet_classnames.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using Colab, we've noticed an issue with hidden files, so uncomment and run the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf /content/imagenet-images/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR8CTAqcoU8k"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this notebook in Colab, you'll want to uncomment and run the following line.\n",
    "\n",
    "If you're running this notebook locally or on a Grace cluster, you can separately install any packages you use. Note: if your device is GPU-compatible, you'll likely get a significant speed-up in running code for this assignment and the next, but it shouldn't be *strictly* necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcNAApKEoe0u"
   },
   "outputs": [],
   "source": [
    "# !pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zn7JBqxT05Q-"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'captum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntegratedGradients\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualization \u001b[38;5;28;01mas\u001b[39;00m viz\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'captum'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiLouhRXolwv"
   },
   "source": [
    "# Useful methods\n",
    "Just run these cells, they contain functions that may be useful for visualizing output in particular.\n",
    "\n",
    "```means``` and ```stds``` are defined in a later cell.\n",
    "\n",
    "Notice that ```visualize_attributions``` accepts a List[] of **numpy arrays**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWaWRpsBoj4C"
   },
   "outputs": [],
   "source": [
    "def unnormalize(img):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        img (Tensor): Tensor image of size (C, H, W), normalized according to means, stds.\n",
    "    Returns:\n",
    "        numpy.ndarray: Unnormalized image of size (H, W, C) in range [0, 1].\n",
    "    \"\"\"\n",
    "    # Convert the Tensor img to a numpy array that can be visualized\n",
    "    img = img.cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * stds[None,None]) + means[None,None]\n",
    "    img = np.clip(img, a_min=0.0, a_max=1.0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGuYeo8vpPCz"
   },
   "outputs": [],
   "source": [
    "def visualize_attributions(predicted_class, predicted_probability, image, attrs, methods, titles):\n",
    "    r\"\"\"\n",
    "    Output n results, each for the same input image.\n",
    "    attrs, methods, titles should be the same length, n.\n",
    "    Args:\n",
    "        predicted_class (str): Name of class predicted\n",
    "        predicted_probability (float): Probability assigned to predicted class by model\n",
    "        image (Tensor): Original image to unnormalize.\n",
    "        attrs (List[]), methods(List[]), titles (List[]):\n",
    "            See https://captum.ai/api/utilities.html for documentation on\n",
    "            captum.attr.visualization.visualize_image_attr.\n",
    "            Elements of attrs should be numpy arrays of shape (224, 224, 3)\n",
    "            Use 'blended_heat_map' or 'original_image' for methods.\n",
    "    \"\"\"\n",
    "    print(f\"Predicted: {predicted_class} with probability: {predicted_probability}\")\n",
    "\n",
    "    original_image = unnormalize(image.cpu().detach())\n",
    "\n",
    "    for attr, method, title in zip(attrs, methods, titles):\n",
    "        if method == 'original_image':\n",
    "            _ = viz.visualize_image_attr(\n",
    "                attr,\n",
    "                original_image,\n",
    "                method=method,\n",
    "                title=title,\n",
    "            )\n",
    "        else:\n",
    "            _ = viz.visualize_image_attr(\n",
    "                attr,\n",
    "                original_image,\n",
    "                method=method,\n",
    "                sign=\"all\",\n",
    "                show_colorbar=True,\n",
    "                title=title,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfdUgM5gszMD"
   },
   "source": [
    "# Implementation\n",
    "Here's where you start coding! You will implement the IntegratedGradients [IG](https://arxiv.org/abs/1703.01365) algorithm in the method ```ig_attribution```. The purpose of this exercise is teach you the IG algorithm, as well as to get you accustomed to accessing gradients in PyTorch, which might be helpful on your projects. Feel free to design any helper functions you might need!\n",
    "\n",
    "I'll bold arguments to the method in this introduction to help you map the description to what you will implement:\n",
    "\n",
    "IG explains why a **model** predicts a **target class** for a given **input image**. It defines a path from a **baseline image** (for us, the all-zero image) to the input image, and integrates the gradient of the model's score with respect to the path. We will use two different methods of computing [Riemann sums](https://en.wikipedia.org/wiki/Riemann_sum) to approximate the integral of this gradient along a linear path.\n",
    "\n",
    "To illustrate the difference between the methods, consider approximating the integral of $f(x)=x^3$ from 0 to 25 with 5 subintervals.\\\n",
    "```method_name='riemann',method_side='left'``` will use \"rectangles\" of\n",
    "$$(height,start,end)=(0^3,0,5),(5^3,5,10),(10^3,10,15),\\dots,(20^3,20,25).$$\n",
    "```method_name='riemann', method_side='right'``` will use \"rectangles\" of\n",
    "$$(height,start,end=(5^3,0,5),(10^3,5,10),(15^3,10,15),\\dots,(25^3,20,25).$$\n",
    "```method_name='quad_linear',method_side='left'``` will use \"rectangles\" of\n",
    "$$(height,start,end)=(0^3,0,1),(1^3,1,4),(4^3,4,9),\\dots,(16^3,16,25).$$\n",
    "```method_name='quad_linear',method_side='right'``` will use \"rectangles\" of\n",
    "$$(height,start,end)=(1^3,0,1),(4^3,1,4),(9^3,4,9),\\dots,(25^3,16,25).$$\n",
    "That is, ```method_name='quad_linear'``` will simply change the rate at which we move along the **still-linear** path; equivalently, it will change the widths of the subintervals to be non-constant.\n",
    "\n",
    "By independence of path, the sum of the values of the integrated gradients $sum(IG)$ should equal $model(input)-model(baseline).$ The output ```delta``` is the difference from approximating, so it should be\n",
    "$$delta=sum(IG) - (model(input)-model(baseline)).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpyDUyOhz937"
   },
   "outputs": [],
   "source": [
    "def ig_attribution(model, inp, baseline, target_class, method_name, method_side, n_steps):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to be explained.\n",
    "        inp (Tensor): The input to the model.\n",
    "        baseline (Tensor): The baseline input to compare with.\n",
    "        target_class (int): Output index for which gradients are computed.\n",
    "        method_name (str): Method for approximating the integral, one of `riemann` or `quad_linear`.\n",
    "        method_side (str): Method for approximating the integral, one of `left` or `right`.\n",
    "        n_steps (int): The number of steps to use in the Riemann approximation.\n",
    "    Returns:\n",
    "        attributions (Tensor): Integrated gradients with respect to each input feature. Same shape as inp.\n",
    "        delta (float): The difference between the total approximated and true integrated gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE:\n",
    "\n",
    "    return attribution, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzaKMU1rr7VC"
   },
   "source": [
    "# Data and Model\n",
    "## Data Processing\n",
    "We've included a few images from the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEJWo0BZsPrf"
   },
   "outputs": [],
   "source": [
    "# Transformations applied to images before passing them to the model\n",
    "# Pretrained normalization based on https://discuss.pytorch.org/t/how-to-preprocess-input-for-pre-trained-networks/683\n",
    "means, stds = [0.485, 0.456, 0.406], [0.485, 0.456, 0.406]\n",
    "means, stds = np.array(means), np.array(stds)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToImage(), # Converts to tensor\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=means, std=stds)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8N855PWsnCA"
   },
   "outputs": [],
   "source": [
    "# Load the small dataset to interpret\n",
    "dataset = torchvision.datasets.ImageFolder(DATA_FOLDER, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekmKVXhIsoqR"
   },
   "outputs": [],
   "source": [
    "# The class names in the dataset are not quite aligned with the class names in the model\n",
    "my_classes = ('bagel', 'barn', 'goldfish', 'mud_turtle')\n",
    "classes = []\n",
    "with open(CLASSNAMES_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        classes.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on034ikgsqDQ"
   },
   "source": [
    "## Load model\n",
    "Import torchvision's ResNet18 model. Be sure to use the pretrained weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_ItrRJDsxnw"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE:\n",
    "### Set the model to eval mode so we can interpret it\n",
    "net = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnoAb2fa0OAA"
   },
   "source": [
    "## Quick Check\n",
    "We'll visualize the images, check their ground truths, and output the pretrained ResNet model's predictions (highest-scoring classes) for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CILFCMzp0NaZ"
   },
   "outputs": [],
   "source": [
    "# Load images and labels from the dataset\n",
    "images_ = torch.stack([dataset[i][0] for i in range(len(my_classes))])\n",
    "labels_ = [dataset[i][1] for i in range(len(my_classes))]\n",
    "\n",
    "# Show images\n",
    "plt.imshow(unnormalize(torchvision.utils.make_grid(images_)))\n",
    "plt.show()\n",
    "print(\"GroundTruth: \", \" \".join(\"%5s\" % my_classes[labels_[j]] for j in range(len(my_classes))))\n",
    "\n",
    "# Predictions\n",
    "\n",
    "outputs_ = net(images_)\n",
    "_, predicted_ = torch.max(outputs_, 1)\n",
    "print(\"Predicted: \", \" \".join(\"%5s\" % classes[predicted_[j]] for j in range(len(my_classes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAm9R6Cu1njU"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ7rXxYe1ttr"
   },
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0mWiavF1pZm"
   },
   "outputs": [],
   "source": [
    "# Current method to be used for captum visualization\n",
    "# See https://captum.ai/api/integrated_gradients.html for documentation,\n",
    "# especially the attribute method.\n",
    "cur_method_captum = 'riemann_left'\n",
    "# Current method to be used for self-implemented visualization\n",
    "cur_method_name, cur_method_side = 'riemann', 'left'\n",
    "\n",
    "# Choose an image to test on\n",
    "ind_ = 3\n",
    "input_tns_ = images_[ind_].unsqueeze(0).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NB89_pU1-EH"
   },
   "source": [
    "## Using the Captum library's IG implementation\n",
    "Use the [documentation](https://captum.ai/api/integrated_gradients.html) for Captum's IG implementation and complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90gHijGz2HMw"
   },
   "outputs": [],
   "source": [
    "def captum_ig(model, inp, baseline, target_class, method, n_steps=50):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to be explained.\n",
    "        inp (Tensor): The input to be explained\n",
    "        baseline (Tensor): The baseline input to compare with.\n",
    "        target_class (int): Output index for which gradients are computed.\n",
    "        method (str): Method for approximating the integral, see Captum documentation for details.\n",
    "        n_steps (int): The number of steps to use in the integral approximation.\n",
    "    Returns:\n",
    "        attributions (Tensor): Integrated gradients with respect to each input feature. Same shape as inp.\n",
    "        delta (float): The difference between the total approximated and true integrated gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE:\n",
    "\n",
    "    return attr_ig, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAdM8NMc27QF"
   },
   "source": [
    "Once you've done this, the following code block should run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRnq5IC33AEz"
   },
   "outputs": [],
   "source": [
    "attr_ig_captum_, delta_captum_ = captum_ig(net, input_tns_, input_tns_ * 0, labels_[ind_], cur_method_captum, 5)\n",
    "# what's the lower bound on error of the approximated integral\n",
    "print(\"Approximation delta: \", abs(delta_captum_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlThQA3R3EXY"
   },
   "source": [
    "## Using your implementation of IG\n",
    "If you've done ```ig_attribution``` right, ```self_ig``` should essentially look like the ```captum_ig```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9UU8XJd3Lmd"
   },
   "outputs": [],
   "source": [
    "def self_ig(model, inp, baseline, target_class, method_name, method_side, n_steps=50):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to be explained.\n",
    "        inp (Tensor): The input to the model.\n",
    "        baseline (Tensor): The baseline input to compare with.\n",
    "        target_class (int): Output index for which gradients are computed.\n",
    "        method_name (str): Method for approximating the integral, one of `riemann` or `quad_linear`.\n",
    "        method_side (str): Method for approximating the integral, one of `left` or `right`.\n",
    "        n_steps (int): The number of steps to use in the Riemann approximation.\n",
    "    Returns:\n",
    "        attributions (Tensor): Integrated gradients with respect to each input feature. Same shape as inp.\n",
    "        delta (float): The difference between the total approximated and true integrated gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE:\n",
    "\n",
    "    return attr_ig, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx5JLQns3cNj"
   },
   "source": [
    "Once you've done this, the following code block should run, and the output should be the same as the approximation delta from ```captum_ig```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q3C9vAc3b5z"
   },
   "outputs": [],
   "source": [
    "attr_ig_, delta_ = self_ig(net, input_tns_, input_tns_ * 0, labels_[ind_], cur_method_name, cur_method_side, 5)\n",
    "print(\"Approximation delta: \", abs(delta_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCl71jOe4anq"
   },
   "source": [
    "# Comparison\n",
    "Choose one of the given 4 images and choose an image of your own.\n",
    "\n",
    "Use Captum (a ```riemann``` method), your implementation of ```riemann```, and your implementation of ```quad_linear``` to explain each image. Be sure to visualize the image and each explanation (```visualize_attributions``` should help) and check the approximation deltas. Comment on the results. It may be interesting to try with the ground truth, the predicted class, or a random class as the target.\n",
    "\n",
    "(You can choose whether to use right or left -- use the same side for a given image, though.)\n",
    "\n",
    "(We used 50 steps. You may use however many steps you'd like -- be consistent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mYNSDrs5d31"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
